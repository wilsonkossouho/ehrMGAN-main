{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• EHR-M-GAN Training on Google Colab\n",
    "\n",
    "**Version:** 2.0 (Production-Ready)\n",
    "\n",
    "**Objectif:** G√©n√©rer des donn√©es m√©dicales synth√©tiques de haute qualit√©\n",
    "\n",
    "**Dataset:** eICU-CRD Demo (1,650 patients)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "**GPU recommand√©:** T4 ou A100\n",
    "\n",
    "**Runtime:** GPU (obligatoire)\n",
    "\n",
    "**Temps estim√©:** 6-8 heures total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ √âtape 1 : Setup Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# V√©rifier acc√®s\n",
    "!ls \"/content/drive/MyDrive/\" | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier GPU disponible\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloner le repository\n",
    "import os\n",
    "\n",
    "# Supprimer si existe d√©j√†\n",
    "!rm -rf /content/ehrMGAN\n",
    "\n",
    "# Cloner\n",
    "!git clone https://github.com/jli0117/ehrMGAN.git /content/ehrMGAN\n",
    "\n",
    "# Se placer dans le dossier\n",
    "%cd /content/ehrMGAN\n",
    "\n",
    "# V√©rifier structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß √âtape 2 : Installation TensorFlow 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITIQUE : Downgrade vers TensorFlow 1.15\n",
    "!pip uninstall -y tensorflow tensorflow-gpu -q\n",
    "!pip install tensorflow-gpu==1.15.5 -q\n",
    "\n",
    "# V√©rifier version\n",
    "import tensorflow as tf\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "assert tf.__version__.startswith('1.15'), \"‚ùå TensorFlow 1.15 requis!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö √âtape 3 : Installer D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer toutes les d√©pendances (versions test√©es)\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "\n",
    "# Core dependencies\n",
    "!pip install numpy==1.19.5 -q\n",
    "!pip install pandas==1.1.5 -q\n",
    "!pip install scipy==1.5.4 -q\n",
    "!pip install scikit-learn==0.24.2 -q\n",
    "!pip install matplotlib==3.3.4 -q\n",
    "!pip install seaborn==0.11.2 -q\n",
    "!pip install h5py==2.10.0 -q\n",
    "!pip install tqdm==4.64.1 -q\n",
    "!pip install pyyaml==5.4.1 -q\n",
    "\n",
    "# PyTorch (pour contrastive loss)\n",
    "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113 -q\n",
    "\n",
    "# V√©rifier imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "print(\"‚úÖ Toutes les d√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ √âtape 4 : Pr√©parer les Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "import shutil\n",
    "\n",
    "# Chemins Drive (√† adapter selon votre structure)\n",
    "DRIVE_DATA = \"/content/drive/MyDrive/ehrMGAN_data/eicu-crd-demo-2.0.1\"\n",
    "DRIVE_FIXES = \"/content/drive/MyDrive/ehrMGAN_fixes\"\n",
    "DRIVE_CHECKPOINT = \"/content/drive/MyDrive/ehrMGAN_checkpoints\"\n",
    "\n",
    "# Chemins locaux\n",
    "LOCAL_DATA = \"/content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/data\"\n",
    "\n",
    "# Cr√©er dossiers\n",
    "os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "os.makedirs(DRIVE_CHECKPOINT, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Dossiers cr√©√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copier donn√©es eICU depuis Drive\n",
    "required_files = [\n",
    "    \"patient.csv.gz\",\n",
    "    \"vitalPeriodic.csv.gz\",\n",
    "    \"infusionDrug.csv.gz\",\n",
    "    \"respiratoryCare.csv.gz\"\n",
    "]\n",
    "\n",
    "print(\"Copie des donn√©es eICU...\")\n",
    "for file in required_files:\n",
    "    src = os.path.join(DRIVE_DATA, file)\n",
    "    dst = os.path.join(LOCAL_DATA, file)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"  ‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå MANQUANT : {file}\")\n",
    "        print(f\"     T√©l√©chargez depuis PhysioNet et uploadez dans Drive\")\n",
    "\n",
    "# V√©rifier\n",
    "!ls -lh {LOCAL_DATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copier fichiers de fix depuis Drive\n",
    "print(\"Application des fixes...\")\n",
    "\n",
    "# visualise.py\n",
    "if os.path.exists(f\"{DRIVE_FIXES}/visualise.py\"):\n",
    "    !cp \"{DRIVE_FIXES}/visualise.py\" /content/ehrMGAN/evaluation_metrics/\n",
    "    print(\"  ‚úÖ visualise.py\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  visualise.py manquant (pourrait causer erreur)\")\n",
    "\n",
    "# utils/\n",
    "if os.path.exists(f\"{DRIVE_FIXES}/utils\"):\n",
    "    !cp -r \"{DRIVE_FIXES}/utils/\"*.py /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/utils/\n",
    "    print(\"  ‚úÖ utils/*.py\")\n",
    "\n",
    "# preprocessing_eicu_complete.py\n",
    "if os.path.exists(f\"{DRIVE_FIXES}/preprocessing_eicu_complete.py\"):\n",
    "    !cp \"{DRIVE_FIXES}/preprocessing_eicu_complete.py\" /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/\n",
    "    print(\"  ‚úÖ preprocessing_eicu_complete.py\")\n",
    "\n",
    "print(\"\\n‚úÖ Fixes appliqu√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ √âtape 5 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se placer dans le dossier preprocessing\n",
    "%cd /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess\n",
    "\n",
    "# Lancer preprocessing\n",
    "!python preprocessing_eicu_complete.py \\\n",
    "  --data_path ./data \\\n",
    "  --output_path ../../data/real/eicu \\\n",
    "  --time_window 24 \\\n",
    "  --min_length 12 \\\n",
    "  --max_length 240 \\\n",
    "  --age_min 18 \\\n",
    "  --verbose\n",
    "\n",
    "# V√©rifier outputs\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fichiers g√©n√©r√©s :\")\n",
    "!ls -lh ../../data/real/eicu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNEL : Fix pickle protocol si erreur\n",
    "import pickle\n",
    "\n",
    "def convert_pickle_protocol(file_path):\n",
    "    \"\"\"Convertir pickle protocol 5 ‚Üí 4\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "        print(f\"‚úÖ Converti : {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur : {e}\")\n",
    "\n",
    "# Convertir tous les .pkl\n",
    "pkl_files = [\n",
    "    \"../../data/real/eicu/vital_sign_24hrs.pkl\",\n",
    "    \"../../data/real/eicu/med_interv_24hrs.pkl\",\n",
    "    \"../../data/real/eicu/statics.pkl\"\n",
    "]\n",
    "\n",
    "for pkl_file in pkl_files:\n",
    "    if os.path.exists(pkl_file):\n",
    "        convert_pickle_protocol(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì √âtape 6 : Configuration Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenir au dossier principal\n",
    "%cd /content/ehrMGAN\n",
    "\n",
    "# Param√®tres optimis√©s pour Colab\n",
    "BATCH_SIZE = 128          # R√©duit pour √©viter OOM\n",
    "NUM_PRE_EPOCHS = 500      # Pretraining VAE\n",
    "NUM_EPOCHS = 800          # Training adversarial\n",
    "CHECKPOINT_FREQ = 50      # Sauvegardes fr√©quentes\n",
    "Z_DIM = 25                # Dimension latente\n",
    "\n",
    "# Cr√©er dossiers de sortie\n",
    "!mkdir -p data/checkpoint\n",
    "!mkdir -p data/fake\n",
    "!mkdir -p logs/visualizations\n",
    "\n",
    "print(f\"\"\"Configuration Training:\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Pretraining Epochs: {NUM_PRE_EPOCHS}\n",
    "- Training Epochs: {NUM_EPOCHS}\n",
    "- Checkpoint Freq: {CHECKPOINT_FREQ}\n",
    "- Latent Dim: {Z_DIM}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activer anti-d√©connexion Colab\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "    function KeepClicking(){\n",
    "        console.log(\"Keeping session alive\");\n",
    "        document.querySelector(\"colab-toolbar-button#connect\").click();\n",
    "    }\n",
    "    setInterval(KeepClicking, 60000);\n",
    "'''))\n",
    "\n",
    "print(\"‚úÖ Anti-d√©connexion activ√© (click toutes les 60s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ √âtape 7 : Phase 1 - Pretraining VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer pretraining VAE\n",
    "!python main_train.py \\\n",
    "  --dataset eicu \\\n",
    "  --data_path ./data/real/eicu \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --num_pre_epochs {NUM_PRE_EPOCHS} \\\n",
    "  --num_epochs 0 \\\n",
    "  --epoch_ckpt_freq {CHECKPOINT_FREQ} \\\n",
    "  --z_dim {Z_DIM} \\\n",
    "  --conditional False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Pretraining VAE termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder checkpoint pretraining dans Drive\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_name = f\"pretraining_complete_{timestamp}\"\n",
    "checkpoint_dir = f\"{DRIVE_CHECKPOINT}/{checkpoint_name}\"\n",
    "\n",
    "shutil.copytree(\"data/checkpoint\", checkpoint_dir, dirs_exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoint sauvegard√© : {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• √âtape 8 : Phase 2 - Training Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer training GAN\n",
    "!python main_train.py \\\n",
    "  --dataset eicu \\\n",
    "  --data_path ./data/real/eicu \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --num_pre_epochs 0 \\\n",
    "  --num_epochs {NUM_EPOCHS} \\\n",
    "  --epoch_ckpt_freq {CHECKPOINT_FREQ} \\\n",
    "  --z_dim {Z_DIM} \\\n",
    "  --conditional False \\\n",
    "  --resume_training True\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Training GAN termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder r√©sultats finaux dans Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_export = f\"{DRIVE_CHECKPOINT}/FINAL_EXPORT_{timestamp}\"\n",
    "os.makedirs(final_export, exist_ok=True)\n",
    "\n",
    "# Copier tout\n",
    "shutil.copytree(\"data/checkpoint\", f\"{final_export}/checkpoints\", dirs_exist_ok=True)\n",
    "shutil.copytree(\"data/fake\", f\"{final_export}/synthetic_data\", dirs_exist_ok=True)\n",
    "shutil.copytree(\"logs\", f\"{final_export}/logs\", dirs_exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Export final complet : {final_export}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä √âtape 9 : Validation R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger donn√©es synth√©tiques\n",
    "import pickle\n",
    "\n",
    "with open(\"data/fake/c_gen_data.pkl\", \"rb\") as f:\n",
    "    c_gen_data = pickle.load(f)\n",
    "\n",
    "with open(\"data/fake/d_gen_data.pkl\", \"rb\") as f:\n",
    "    d_gen_data = pickle.load(f)\n",
    "\n",
    "# Charger donn√©es r√©elles\n",
    "with open(\"data/real/eicu/vital_sign_24hrs.pkl\", \"rb\") as f:\n",
    "    c_real_data = pickle.load(f)\n",
    "\n",
    "with open(\"data/real/eicu/med_interv_24hrs.pkl\", \"rb\") as f:\n",
    "    d_real_data = pickle.load(f)\n",
    "\n",
    "print(f\"Synth√©tiques - Continues: {c_gen_data.shape}, Discr√®tes: {d_gen_data.shape}\")\n",
    "print(f\"R√©elles - Continues: {c_real_data.shape}, Discr√®tes: {d_real_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser comparaisons\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "num_samples = 5\n",
    "feature_names = ['Heart Rate', 'SpO2', 'SBP', 'DBP', 'Temp', 'Resp Rate', 'GCS']\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(14, 10))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # R√©el\n",
    "    axes[i, 0].plot(c_real_data[i], alpha=0.7, linewidth=0.8)\n",
    "    axes[i, 0].set_title(f\"Patient {i+1} - R√©el\")\n",
    "    axes[i, 0].set_ylabel(\"Valeur normalis√©e\")\n",
    "    \n",
    "    # Synth√©tique\n",
    "    axes[i, 1].plot(c_gen_data[i], alpha=0.7, linewidth=0.8)\n",
    "    axes[i, 1].set_title(f\"Patient {i+1} - Synth√©tique\")\n",
    "    \n",
    "    if i == num_samples - 1:\n",
    "        axes[i, 0].set_xlabel(\"Heure\")\n",
    "        axes[i, 1].set_xlabel(\"Heure\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_CHECKPOINT}/comparison_trajectories.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer MMD (Maximum Mean Discrepancy)\n",
    "from evaluation_metrics.max_mean_discrepency import mmd_rbf\n",
    "import numpy as np\n",
    "\n",
    "mmd_scores = []\n",
    "print(\"Calcul MMD par feature:\\n\")\n",
    "print(f\"{'Feature':<15} {'MMD':<10}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for feat_idx in range(c_real_data.shape[2]):\n",
    "    real_feat = c_real_data[:, :, feat_idx].reshape(-1)\n",
    "    gen_feat = c_gen_data[:, :, feat_idx].reshape(-1)\n",
    "    \n",
    "    mmd = mmd_rbf(real_feat, gen_feat)\n",
    "    mmd_scores.append(mmd)\n",
    "    print(f\"{feature_names[feat_idx]:<15} {mmd:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*25)\n",
    "print(f\"MMD moyen : {np.mean(mmd_scores):.6f}\")\n",
    "print(\"\\nüéØ Cible : < 0.05 (excellent), < 0.10 (bon)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminative Score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Pr√©parer donn√©es\n",
    "X_real = c_real_data.reshape(len(c_real_data), -1)\n",
    "X_gen = c_gen_data.reshape(len(c_gen_data), -1)\n",
    "\n",
    "X = np.vstack([X_real, X_gen])\n",
    "y = np.hstack([np.ones(len(X_real)), np.zeros(len(X_gen))])\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entra√Æner\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dire\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nDiscriminative Score: {accuracy:.4f}\")\n",
    "print(\"üéØ Cible : ~0.50 (id√©al = indistinguable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er rapport final\n",
    "from datetime import datetime\n",
    "\n",
    "report = f\"\"\"\n",
    "{'='*60}\n",
    "EHR-M-GAN TRAINING REPORT\n",
    "{'='*60}\n",
    "\n",
    "Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Dataset: eICU-CRD Demo\n",
    "GPU: {!nvidia-smi --query-gpu=name --format=csv,noheader | head -1}\n",
    "\n",
    "{'='*60}\n",
    "CONFIGURATION\n",
    "{'='*60}\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Pretraining Epochs: {NUM_PRE_EPOCHS}\n",
    "- Training Epochs: {NUM_EPOCHS}\n",
    "- Latent Dimension: {Z_DIM}\n",
    "- Checkpoint Frequency: {CHECKPOINT_FREQ}\n",
    "\n",
    "{'='*60}\n",
    "DONN√âES\n",
    "{'='*60}\n",
    "- Patients trait√©s: {len(c_real_data)}\n",
    "- Fen√™tre temporelle: 24 heures\n",
    "- Features continues: {c_real_data.shape[2]} (vital signs)\n",
    "- Features discr√®tes: {d_real_data.shape[2]} (interventions)\n",
    "\n",
    "{'='*60}\n",
    "M√âTRIQUES QUALIT√â\n",
    "{'='*60}\n",
    "- MMD moyen: {np.mean(mmd_scores):.6f} {'‚úÖ' if np.mean(mmd_scores) < 0.10 else '‚ö†Ô∏è'}\n",
    "- Discriminative Score: {accuracy:.4f} {'‚úÖ' if 0.45 <= accuracy <= 0.55 else '‚ö†Ô∏è'}\n",
    "\n",
    "MMD par feature:\n",
    "\"\"\"\n",
    "\n",
    "for fname, mmd in zip(feature_names, mmd_scores):\n",
    "    report += f\"  - {fname:<15}: {mmd:.6f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "{'='*60}\n",
    "FICHIERS G√âN√âR√âS\n",
    "{'='*60}\n",
    "- Checkpoints: {final_export}/checkpoints/\n",
    "- Synthetic Data: {final_export}/synthetic_data/\n",
    "- Logs: {final_export}/logs/\n",
    "- Visualizations: {DRIVE_CHECKPOINT}/comparison_trajectories.png\n",
    "\n",
    "{'='*60}\n",
    "STATUT\n",
    "{'='*60}\n",
    "‚úÖ Training compl√©t√© avec succ√®s\n",
    "‚úÖ Donn√©es synth√©tiques g√©n√©r√©es\n",
    "‚úÖ Sauvegarde Drive effectu√©e\n",
    "\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Sauvegarder rapport\n",
    "with open(f\"{final_export}/TRAINING_REPORT.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n‚úÖ Rapport sauvegard√© : {final_export}/TRAINING_REPORT.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier que tout est sauvegard√©\n",
    "import os\n",
    "\n",
    "checklist = [\n",
    "    (f\"{final_export}/checkpoints\", \"Checkpoints mod√®le\"),\n",
    "    (f\"{final_export}/synthetic_data\", \"Donn√©es synth√©tiques\"),\n",
    "    (f\"{final_export}/logs\", \"Logs training\"),\n",
    "    (f\"{final_export}/TRAINING_REPORT.txt\", \"Rapport final\"),\n",
    "    (f\"{DRIVE_CHECKPOINT}/comparison_trajectories.png\", \"Visualisations\")\n",
    "]\n",
    "\n",
    "print(\"V√©rification des fichiers:\\n\")\n",
    "all_ok = True\n",
    "for path, desc in checklist:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {desc}: {path}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_ok:\n",
    "    print(\"‚úÖ TOUT EST SAUVEGARD√â - Vous pouvez fermer le notebook\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ATTENTION - Fichiers manquants, v√©rifiez les erreurs ci-dessus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ FIN DU TRAINING\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "\n",
    "1. **T√©l√©charger les r√©sultats** depuis Drive\n",
    "2. **Analyser les m√©triques** (MMD, Discriminative Score)\n",
    "3. **Valider downstream** (mod√®les pr√©dictifs)\n",
    "4. **Publier r√©sultats** (article, GitHub)\n",
    "\n",
    "### Support:\n",
    "\n",
    "- **Documentation**: [COLAB_SETUP_GUIDE.md](COLAB_SETUP_GUIDE.md)\n",
    "- **GitHub**: https://github.com/jli0117/ehrMGAN\n",
    "- **Article**: https://arxiv.org/abs/2112.12047\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook cr√©√© par [Votre Nom] - Version 2.0 (F√©vrier 2026)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
