# üöÄ Guide Complet : EHR-M-GAN sur Google Colab

**Version :** 2.0 (Post-Production)
**Date :** F√©vrier 2026
**Bas√© sur :** 48h de d√©veloppement technique et r√©solution de bugs

---

## üìã TABLE DES MATI√àRES

1. [Vue d'ensemble](#vue-densemble)
2. [Pr√©requis](#pr√©requis)
3. [Configuration initiale](#configuration-initiale)
4. [Installation des d√©pendances](#installation-des-d√©pendances)
5. [T√©l√©chargement des donn√©es](#t√©l√©chargement-des-donn√©es)
6. [Preprocessing](#preprocessing)
7. [Entra√Ænement](#entra√Ænement)
8. [Gestion des erreurs courantes](#gestion-des-erreurs-courantes)
9. [Optimisations Colab](#optimisations-colab)

---

## üéØ VUE D'ENSEMBLE

### Pourquoi ce guide ?

Ce tutoriel r√©sout **tous les probl√®mes rencontr√©s** lors du d√©ploiement local :
- ‚úÖ Incompatibilit√©s TensorFlow 1.x vs 2.x
- ‚úÖ Modules manquants (`visualise.py`, `utils/`)
- ‚úÖ Bugs de pickle protocol
- ‚úÖ Pipeline preprocessing fragment√©
- ‚úÖ Crashes √† epoch 99

### Temps estim√©
- **Setup complet** : 15-20 minutes
- **Preprocessing** : 5-10 minutes
- **Entra√Ænement** : 6-8 heures (pretraining + adversarial)

### Avertissements Colab
‚ö†Ô∏è **Limitations gratuites** :
- 12h maximum de runtime continu
- D√©connexion al√©atoire si inactif
- GPU non garanti (Tesla K80/T4 al√©atoire)

üí° **Recommandations** :
- Utiliser **Colab Pro** (10‚Ç¨/mois) pour :
  - Runtime 24h
  - GPU prioritaire (A100 possible)
  - Plus de RAM (25 GB vs 12 GB)

---

## üì¶ PR√âREQUIS

### Avant de commencer

1. **Compte Google** avec Google Drive
2. **Acc√®s PhysioNet** (gratuit) :
   - Cr√©er compte sur https://physionet.org/
   - Compl√©ter formation CITI (2h)
   - Signer Data Use Agreement pour eICU-CRD Demo

3. **T√©l√©charger eICU-CRD Demo** (m√©thode recommand√©e) :
   ```bash
   # Sur votre machine locale
   wget -r -N -c -np --user VOTRE_USERNAME --ask-password \
     https://physionet.org/files/eicu-crd-demo/2.0.1/
   ```
   **OU** utiliser l'interface web PhysioNet (plus simple)

4. **Uploader sur Google Drive** :
   ```
   Mon Drive/
   ‚îî‚îÄ‚îÄ ehrMGAN_data/
       ‚îî‚îÄ‚îÄ eicu-crd-demo-2.0.1/
           ‚îú‚îÄ‚îÄ diagnosis.csv.gz
           ‚îú‚îÄ‚îÄ lab.csv.gz
           ‚îú‚îÄ‚îÄ medication.csv.gz
           ‚îú‚îÄ‚îÄ nurseCharting.csv.gz
           ‚îú‚îÄ‚îÄ patient.csv.gz
           ‚îú‚îÄ‚îÄ vitalPeriodic.csv.gz
           ‚îî‚îÄ‚îÄ ... (autres fichiers)
   ```

---

## üîß CONFIGURATION INITIALE

### √âtape 1 : Cr√©er le notebook Colab

1. Aller sur https://colab.research.google.com/
2. **Fichier** ‚Üí **Nouveau notebook**
3. **Nom** : `EHR_M_GAN_Training.ipynb`
4. **Runtime** ‚Üí **Modifier le type de runtime** ‚Üí **GPU** (T4 recommand√©)

### √âtape 2 : Monter Google Drive

```python
# Cellule 1 : Monter Drive
from google.colab import drive
drive.mount('/content/drive')

# V√©rifier acc√®s
!ls "/content/drive/MyDrive/"
```

**‚úÖ Sortie attendue** : Liste de vos dossiers Drive (dont `ehrMGAN_data`)

### √âtape 3 : Cloner le repository

```python
# Cellule 2 : Cloner le projet
import os

# Supprimer si existe d√©j√† (pour re-runs)
!rm -rf /content/ehrMGAN

# Cloner depuis GitHub
!git clone https://github.com/jli0117/ehrMGAN.git /content/ehrMGAN

# Se placer dans le dossier
%cd /content/ehrMGAN

# V√©rifier structure
!ls -la
```

**‚úÖ Sortie attendue** :
```
main_train.py
m3gan.py
networks.py
preprocessing_physionet-main/
evaluation_metrics/
...
```

---

## üìö INSTALLATION DES D√âPENDANCES

### ‚ö†Ô∏è CRITIQUE : Configuration TensorFlow

**Le code utilise TensorFlow 1.x avec des APIs obsol√®tes**. Sur Colab (qui vient avec TF2), il faut :

```python
# Cellule 3 : Downgrade TensorFlow
!pip uninstall -y tensorflow tensorflow-gpu

# Installer TensorFlow 1.15 (derni√®re version compatible)
!pip install tensorflow-gpu==1.15.5

# V√©rifier version
import tensorflow as tf
print(f"TensorFlow version: {tf.__version__}")
# Attendu : 1.15.5
```

### Installation compl√®te

```python
# Cellule 4 : Installer toutes les d√©pendances
!pip install --upgrade pip setuptools wheel

# D√©pendances core (versions exactes test√©es)
!pip install numpy==1.19.5
!pip install pandas==1.1.5
!pip install scipy==1.5.4
!pip install scikit-learn==0.24.2
!pip install matplotlib==3.3.4
!pip install seaborn==0.11.2
!pip install h5py==2.10.0
!pip install tqdm==4.64.1
!pip install pyyaml==5.4.1

# PyTorch (pour contrastive loss uniquement)
!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113

# V√©rifier imports critiques
import numpy as np
import pandas as pd
import tensorflow as tf
import torch
print("‚úÖ Toutes les d√©pendances install√©es")
```

**‚è±Ô∏è Temps** : 3-5 minutes

---

## ü©∫ T√âL√âCHARGEMENT DES DONN√âES

### Option A : Depuis Google Drive (recommand√©)

```python
# Cellule 5 : Copier donn√©es depuis Drive
import os
import shutil

# Chemins
DRIVE_DATA = "/content/drive/MyDrive/ehrMGAN_data/eicu-crd-demo-2.0.1"
LOCAL_DATA = "/content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/data"

# Cr√©er dossier local
os.makedirs(LOCAL_DATA, exist_ok=True)

# Copier fichiers n√©cessaires (pas tout pour gagner du temps)
required_files = [
    "patient.csv.gz",
    "vitalPeriodic.csv.gz",
    "infusionDrug.csv.gz",
    "respiratoryCare.csv.gz"
]

for file in required_files:
    src = os.path.join(DRIVE_DATA, file)
    dst = os.path.join(LOCAL_DATA, file)
    if os.path.exists(src):
        shutil.copy2(src, dst)
        print(f"‚úÖ Copi√© : {file}")
    else:
        print(f"‚ùå MANQUANT : {file}")
        print(f"   Veuillez t√©l√©charger depuis PhysioNet")

# V√©rifier
!ls -lh {LOCAL_DATA}
```

### Option B : T√©l√©chargement direct (n√©cessite credentials)

```python
# Cellule 5bis : T√©l√©chargement direct PhysioNet
# ‚ö†Ô∏è NE FONCTIONNE QUE SI VOUS AVEZ L'ACC√àS

!wget -r -N -c -np --user VOTRE_USERNAME --ask-password \
  -P /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/data \
  https://physionet.org/files/eicu-crd-demo/2.0.1/
```

---

## üîÑ PREPROCESSING

### √âtape 1 : T√©l√©charger les fichiers manquants

**üõ†Ô∏è Fix pour les bugs connus** : Le repository GitHub a des fichiers manquants/incomplets.

```python
# Cellule 6 : T√©l√©charger fichiers de fix depuis Drive
# (Vous devez avoir upload√© les versions corrig√©es)

FIXES_DIR = "/content/drive/MyDrive/ehrMGAN_fixes"

# Copier visualise.py (recr√©√© from scratch)
!cp "{FIXES_DIR}/visualise.py" /content/ehrMGAN/evaluation_metrics/

# Copier utils corrig√©s
!cp "{FIXES_DIR}/utils/"*.py /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/utils/

# Copier script de preprocessing unifi√©
!cp "{FIXES_DIR}/preprocessing_eicu_complete.py" /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess/

print("‚úÖ Fichiers de fix appliqu√©s")
```

**üìÅ Structure des fixes √† pr√©parer** (dans votre Drive) :
```
Mon Drive/ehrMGAN_fixes/
‚îú‚îÄ‚îÄ visualise.py                      # Module de visualisation recr√©√©
‚îú‚îÄ‚îÄ preprocessing_eicu_complete.py    # Script unifi√©
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ utils.py                      # Utilitaires de base
    ‚îî‚îÄ‚îÄ pat_utils.py                  # Filtrage patients
```

### √âtape 2 : Ex√©cuter le preprocessing

```python
# Cellule 7 : Preprocessing complet
%cd /content/ehrMGAN/preprocessing_physionet-main/eicu_preprocess

# Lancer le script unifi√©
!python preprocessing_eicu_complete.py \
  --data_path ./data \
  --output_path ../../data/real/eicu \
  --time_window 24 \
  --min_length 12 \
  --max_length 240 \
  --age_min 18 \
  --verbose

# V√©rifier outputs
!ls -lh ../../data/real/eicu/
```

**‚úÖ Sortie attendue** :
```
vital_sign_24hrs.pkl          # ~15 MB
med_interv_24hrs.pkl          # ~5 MB
statics.pkl                   # ~200 KB
norm_stats.npz                # ~5 KB

Total : 1,650 patients trait√©s
```

**‚è±Ô∏è Temps** : 5-10 minutes

### üêõ Fix pickle protocol (si erreur)

Si vous voyez : `ValueError: unsupported pickle protocol: 5`

```python
# Cellule 7bis : Convertir pickle protocol
import pickle
import os

def convert_pickle_protocol(input_file, output_file):
    """Convertir pickle protocol 5 ‚Üí 4 pour Python 3.7"""
    with open(input_file, 'rb') as f:
        data = pickle.load(f)
    with open(output_file, 'wb') as f:
        pickle.dump(data, f, protocol=4)
    print(f"‚úÖ Converti : {output_file}")

# Convertir tous les .pkl
pkl_files = [
    "../../data/real/eicu/vital_sign_24hrs.pkl",
    "../../data/real/eicu/med_interv_24hrs.pkl",
    "../../data/real/eicu/statics.pkl"
]

for pkl_file in pkl_files:
    if os.path.exists(pkl_file):
        convert_pickle_protocol(pkl_file, pkl_file)
```

---

## üéì ENTRA√éNEMENT

### √âtape 1 : Configuration des hyperparam√®tres

```python
# Cellule 8 : Configuration training
%cd /content/ehrMGAN

# Param√®tres optimis√©s pour Colab
BATCH_SIZE = 128          # R√©duit pour √©viter OOM (original: 256)
NUM_PRE_EPOCHS = 500      # Pretraining VAE
NUM_EPOCHS = 800          # Training adversarial
CHECKPOINT_FREQ = 50      # Sauvegardes fr√©quentes (original: 100)

# Cr√©er dossiers de sortie
!mkdir -p data/checkpoint
!mkdir -p data/fake
!mkdir -p logs/visualizations

# Sauvegardes dans Drive (persistence)
DRIVE_CHECKPOINT = "/content/drive/MyDrive/ehrMGAN_checkpoints"
!mkdir -p "{DRIVE_CHECKPOINT}"
```

### √âtape 2 : Lancer le pretraining VAE

```python
# Cellule 9 : Phase 1 - Pretraining VAE
!python main_train.py \
  --dataset eicu \
  --data_path ./data/real/eicu \
  --batch_size {BATCH_SIZE} \
  --num_pre_epochs {NUM_PRE_EPOCHS} \
  --num_epochs 0 \
  --epoch_ckpt_freq {CHECKPOINT_FREQ} \
  --z_dim 25 \
  --conditional False

# Copier checkpoint dans Drive
!cp -r data/checkpoint/* "{DRIVE_CHECKPOINT}/"
print("‚úÖ Pretraining VAE termin√©")
```

**‚è±Ô∏è Temps** : 2-3 heures (GPU T4)

**üìä Monitoring** :
```python
# Cellule 9bis : Visualiser loss curves
import matplotlib.pyplot as plt
import pandas as pd

# Lire logs (si disponibles)
log_file = "logs/training_log.csv"
if os.path.exists(log_file):
    df = pd.read_csv(log_file)
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(df['epoch'], df['vae_loss'])
    plt.title('VAE Loss')
    plt.xlabel('Epoch')

    plt.subplot(1, 2, 2)
    plt.plot(df['epoch'], df['reconstruction_loss'])
    plt.title('Reconstruction Loss')
    plt.xlabel('Epoch')

    plt.tight_layout()
    plt.savefig(f"{DRIVE_CHECKPOINT}/pretraining_curves.png", dpi=150)
    plt.show()
```

### √âtape 3 : Training adversarial

```python
# Cellule 10 : Phase 2 - Training GAN
!python main_train.py \
  --dataset eicu \
  --data_path ./data/real/eicu \
  --batch_size {BATCH_SIZE} \
  --num_pre_epochs 0 \
  --num_epochs {NUM_EPOCHS} \
  --epoch_ckpt_freq {CHECKPOINT_FREQ} \
  --z_dim 25 \
  --conditional False \
  --resume_training True

# Sauvegarder r√©sultats finaux
!cp -r data/checkpoint/* "{DRIVE_CHECKPOINT}/"
!cp -r data/fake/* "{DRIVE_CHECKPOINT}/"
!cp -r logs/* "{DRIVE_CHECKPOINT}/"
print("‚úÖ Training GAN termin√©")
```

**‚è±Ô∏è Temps** : 4-6 heures (GPU T4)

### üõ°Ô∏è Protection contre d√©connexion

```python
# Cellule 10bis : Script anti-d√©connexion
from IPython.display import display, Javascript
import time

def keep_alive():
    """Emp√™che Colab de se d√©connecter"""
    display(Javascript('''
        function KeepClicking(){
            console.log("Keeping session alive");
            document.querySelector("colab-toolbar-button#connect").click();
        }
        setInterval(KeepClicking, 60000);
    '''))

keep_alive()
print("‚úÖ Anti-d√©connexion activ√© (click toutes les 60s)")
```

### üì∏ Checkpointing intelligent

```python
# Cellule 11 : Fonction de sauvegarde robuste
import shutil
from datetime import datetime

def save_checkpoint_to_drive(epoch):
    """Sauvegarde checkpoint avec timestamp"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_name = f"checkpoint_epoch{epoch}_{timestamp}"

    # Cr√©er dossier dat√©
    checkpoint_dir = f"{DRIVE_CHECKPOINT}/{checkpoint_name}"
    os.makedirs(checkpoint_dir, exist_ok=True)

    # Copier fichiers
    shutil.copytree("data/checkpoint", f"{checkpoint_dir}/checkpoint", dirs_exist_ok=True)
    shutil.copytree("data/fake", f"{checkpoint_dir}/fake", dirs_exist_ok=True)
    shutil.copytree("logs", f"{checkpoint_dir}/logs", dirs_exist_ok=True)

    print(f"‚úÖ Sauvegard√© : {checkpoint_dir}")

    # Garder seulement les 3 derniers checkpoints (√©conomie espace)
    all_checkpoints = sorted([d for d in os.listdir(DRIVE_CHECKPOINT) if d.startswith("checkpoint_")])
    if len(all_checkpoints) > 3:
        for old_ckpt in all_checkpoints[:-3]:
            shutil.rmtree(f"{DRIVE_CHECKPOINT}/{old_ckpt}")
            print(f"üóëÔ∏è Supprim√© ancien : {old_ckpt}")

# Appeler toutes les 100 epochs
# (√† int√©grer dans le code de training si possible)
```

---

## üî• GESTION DES ERREURS COURANTES

### Erreur 1 : `ResourceExhaustedError` (OOM)

**Sympt√¥mes** :
```
tensorflow.python.framework.errors_impl.ResourceExhaustedError:
OOM when allocating tensor with shape [256,24,128]
```

**Solution** :
```python
# R√©duire batch size
BATCH_SIZE = 64  # Au lieu de 128

# OU lib√©rer m√©moire GPU
import tensorflow as tf
from numba import cuda

cuda.select_device(0)
cuda.close()
tf.keras.backend.clear_session()

# Red√©marrer runtime si n√©cessaire
```

### Erreur 2 : `ModuleNotFoundError: visualise`

**Sympt√¥mes** :
```
ModuleNotFoundError: No module named 'evaluation_metrics.visualise'
```

**Solution** :
```python
# V√©rifier pr√©sence du fichier
!ls -la evaluation_metrics/visualise.py

# Si absent, t√©l√©charger depuis Drive
!cp "/content/drive/MyDrive/ehrMGAN_fixes/visualise.py" evaluation_metrics/

# V√©rifier import
import sys
sys.path.append('/content/ehrMGAN')
from evaluation_metrics import visualise
print("‚úÖ Module visualise import√©")
```

### Erreur 3 : `ValueError: unsupported pickle protocol`

**Sympt√¥mes** :
```
ValueError: unsupported pickle protocol: 5
```

**Solution** : Voir section "Fix pickle protocol" dans Preprocessing

### Erreur 4 : Crash √† epoch 99

**Sympt√¥mes** :
```
ValueError: need at least one array to stack
File: m3gan.py, line 487 in np.vstack()
```

**Cause** : Bug dans le code original (listes vides)

**Solution** :
```python
# √âditer m3gan.py ligne 487
# AVANT :
# fake_c = np.vstack(fake_c_epoch)

# APR√àS :
if len(fake_c_epoch) > 0:
    fake_c = np.vstack(fake_c_epoch)
else:
    fake_c = np.array([])  # G√©rer cas vide
```

**Fix automatique** :
```python
# Cellule Fix : Patcher m3gan.py
!sed -i '487s/.*/        if len(fake_c_epoch) > 0:\n            fake_c = np.vstack(fake_c_epoch)\n        else:\n            fake_c = np.array([])/' m3gan.py

print("‚úÖ Bug epoch 99 patch√©")
```

### Erreur 5 : `No GPU available`

**Sympt√¥mes** :
```
WARNING: No GPU found. Training will be slow.
```

**Solution** :
```python
# V√©rifier GPU
!nvidia-smi

# Si vide, changer runtime :
# Runtime ‚Üí Change runtime type ‚Üí GPU (T4)

# Puis restart runtime
```

---

## ‚ö° OPTIMISATIONS COLAB

### 1. Mixed Precision Training (gain 2x vitesse)

```python
# Cellule Optim 1 : Activer mixed precision
import tensorflow as tf

# Pour TensorFlow 1.15 (limit√©)
os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'

# V√©rifier
print("‚úÖ Mixed precision activ√©e (si GPU compatible)")
```

### 2. XLA Compilation (gain 1.5x vitesse)

```python
# Cellule Optim 2 : Activer XLA
import tensorflow as tf

# Activer XLA JIT
tf.config.optimizer.set_jit(True)

# OU via variables d'environnement
os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'

print("‚úÖ XLA compilation activ√©e")
```

### 3. Monitoring ressources

```python
# Cellule Optim 3 : Dashboard ressources
!pip install gputil psutil

import GPUtil
import psutil
import time

def print_resources():
    """Affiche CPU, RAM, GPU en temps r√©el"""
    # CPU
    cpu_percent = psutil.cpu_percent(interval=1)

    # RAM
    ram = psutil.virtual_memory()
    ram_used = ram.used / (1024**3)
    ram_total = ram.total / (1024**3)

    # GPU
    gpus = GPUtil.getGPUs()
    if gpus:
        gpu = gpus[0]
        gpu_load = gpu.load * 100
        gpu_mem = gpu.memoryUsed
        gpu_temp = gpu.temperature

        print(f"CPU: {cpu_percent:.1f}% | RAM: {ram_used:.1f}/{ram_total:.1f} GB")
        print(f"GPU: {gpu_load:.1f}% | VRAM: {gpu_mem:.0f} MB | Temp: {gpu_temp}¬∞C")
    else:
        print(f"CPU: {cpu_percent:.1f}% | RAM: {ram_used:.1f}/{ram_total:.1f} GB")
        print("GPU: Non disponible")

# Afficher toutes les 5 minutes pendant training
import threading

def monitor_loop():
    while True:
        print_resources()
        time.sleep(300)  # 5 minutes

monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
monitor_thread.start()
print("‚úÖ Monitoring activ√© (refresh 5min)")
```

### 4. Compression des checkpoints

```python
# Cellule Optim 4 : Compresser checkpoints avant upload Drive
import tarfile

def compress_checkpoint(epoch):
    """Compresser checkpoint en .tar.gz (√©conomie 70% espace)"""
    checkpoint_dir = f"data/checkpoint"
    output_file = f"{DRIVE_CHECKPOINT}/checkpoint_epoch{epoch}.tar.gz"

    with tarfile.open(output_file, "w:gz") as tar:
        tar.add(checkpoint_dir, arcname="checkpoint")

    print(f"‚úÖ Compress√© : {output_file} ({os.path.getsize(output_file) / 1024**2:.1f} MB)")

# Utiliser √† chaque sauvegarde
```

---

## üìä VALIDATION R√âSULTATS

### √âtape 1 : Charger donn√©es synth√©tiques

```python
# Cellule Valid 1 : Charger donn√©es g√©n√©r√©es
import numpy as np
import pickle

# Charger donn√©es synth√©tiques
with open("data/fake/c_gen_data.pkl", "rb") as f:
    c_gen_data = pickle.load(f)  # Shape: (1650, 24, 7)

with open("data/fake/d_gen_data.pkl", "rb") as f:
    d_gen_data = pickle.load(f)  # Shape: (1650, 24, 3)

# Charger donn√©es r√©elles (pour comparaison)
with open("data/real/eicu/vital_sign_24hrs.pkl", "rb") as f:
    c_real_data = pickle.load(f)

with open("data/real/eicu/med_interv_24hrs.pkl", "rb") as f:
    d_real_data = pickle.load(f)

print(f"Synth√©tiques - Continues: {c_gen_data.shape}, Discr√®tes: {d_gen_data.shape}")
print(f"R√©elles - Continues: {c_real_data.shape}, Discr√®tes: {d_real_data.shape}")
```

### √âtape 2 : Visualiser comparaisons

```python
# Cellule Valid 2 : Visualiser trajectoires
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")

# Comparer 5 patients al√©atoires
num_samples = 5
feature_names = ['Heart Rate', 'SpO2', 'SBP', 'DBP', 'Temp', 'Resp Rate', 'GCS']

fig, axes = plt.subplots(num_samples, 2, figsize=(14, 10))

for i in range(num_samples):
    # Donn√©es r√©elles
    axes[i, 0].plot(c_real_data[i], alpha=0.7)
    axes[i, 0].set_title(f"Patient {i+1} - R√©el")
    axes[i, 0].set_ylabel("Valeur normalis√©e")

    # Donn√©es synth√©tiques
    axes[i, 1].plot(c_gen_data[i], alpha=0.7)
    axes[i, 1].set_title(f"Patient {i+1} - Synth√©tique")

    if i == num_samples - 1:
        axes[i, 0].set_xlabel("Heure")
        axes[i, 1].set_xlabel("Heure")

plt.tight_layout()
plt.savefig(f"{DRIVE_CHECKPOINT}/comparison_trajectories.png", dpi=150)
plt.show()
```

### √âtape 3 : M√©triques quantitatives

```python
# Cellule Valid 3 : Calculer MMD (Maximum Mean Discrepancy)
from evaluation_metrics.max_mean_discrepency import mmd_rbf

# Calculer MMD pour chaque feature
mmd_scores = []
for feat_idx in range(c_real_data.shape[2]):
    real_feat = c_real_data[:, :, feat_idx].reshape(-1)
    gen_feat = c_gen_data[:, :, feat_idx].reshape(-1)

    mmd = mmd_rbf(real_feat, gen_feat)
    mmd_scores.append(mmd)
    print(f"{feature_names[feat_idx]:<15} MMD: {mmd:.6f}")

print(f"\nMMD moyen : {np.mean(mmd_scores):.6f}")
print("üéØ Cible : < 0.05 (excellent), < 0.10 (bon)")
```

### √âtape 4 : Discriminative Score

```python
# Cellule Valid 4 : Post-hoc discriminator
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Pr√©parer donn√©es
X_real = c_real_data.reshape(len(c_real_data), -1)
X_gen = c_gen_data.reshape(len(c_gen_data), -1)

X = np.vstack([X_real, X_gen])
y = np.hstack([np.ones(len(X_real)), np.zeros(len(X_gen))])

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entra√Æner classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Pr√©dire
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Discriminative Score: {accuracy:.4f}")
print("üéØ Cible : ~0.50 (id√©al = indistinguable)")
```

---

## üíæ SAUVEGARDE FINALE

```python
# Cellule Final : Export complet
import shutil
from datetime import datetime

# Cr√©er archive finale
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
final_export = f"{DRIVE_CHECKPOINT}/FINAL_EXPORT_{timestamp}"
os.makedirs(final_export, exist_ok=True)

# Copier tout
shutil.copytree("data/checkpoint", f"{final_export}/checkpoints", dirs_exist_ok=True)
shutil.copytree("data/fake", f"{final_export}/synthetic_data", dirs_exist_ok=True)
shutil.copytree("logs", f"{final_export}/logs", dirs_exist_ok=True)

# Cr√©er README
with open(f"{final_export}/README.txt", "w") as f:
    f.write(f"""
EHR-M-GAN Training Results
=========================
Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Dataset: eICU-CRD Demo (1,650 patients)
Epochs: Pretraining {NUM_PRE_EPOCHS} + Adversarial {NUM_EPOCHS}
Batch Size: {BATCH_SIZE}
GPU: {!nvidia-smi | grep "Tesla"}

Fichiers:
- checkpoints/       : Mod√®les sauvegard√©s
- synthetic_data/    : 1,650 patients synth√©tiques
- logs/              : Courbes de loss, visualisations

M√©triques:
- MMD: {np.mean(mmd_scores):.6f}
- Discriminative Score: {accuracy:.4f}

Pour r√©utiliser:
python main_train.py --resume_training True --checkpoint_path {final_export}/checkpoints
""")

print(f"‚úÖ Export final complet : {final_export}")
print(f"üì¶ Taille totale : {shutil.disk_usage(final_export).used / 1024**3:.2f} GB")
```

---

## üéì CHECKLIST FINALE

### Avant de fermer Colab

- [ ] V√©rifier que tous les checkpoints sont dans Drive
- [ ] T√©l√©charger `FINAL_EXPORT_*` localement (backup)
- [ ] V√©rifier que `synthetic_data/` contient bien les .pkl
- [ ] Sauvegarder les graphiques de validation
- [ ] Noter les m√©triques finales (MMD, Discriminative Score)
- [ ] Exporter le notebook `.ipynb` dans Drive

### Prochaines √©tapes

1. **Analyse approfondie** :
   - Corr√©lations crois√©es (Pearson)
   - Tests statistiques (KS-test, Chi-square)
   - Visualisations avanc√©es (t-SNE, PCA)

2. **Validation downstream** :
   - Entra√Æner mod√®les pr√©dictifs sur synth√©tiques
   - Comparer performances vs r√©elles
   - Publier r√©sultats

3. **Scaling** :
   - Demander acc√®s eICU complet (200k patients)
   - Tester sur MIMIC-III
   - Optimiser architecture (TF2 migration)

---

## üìû SUPPORT

### Probl√®mes courants

| Erreur | Lien Solution |
|--------|---------------|
| OOM GPU | [Section Erreur 1](#erreur-1-resourceexhaustederror-oom) |
| Module manquant | [Section Erreur 2](#erreur-2-modulenotfounderror-visualise) |
| Pickle protocol | [Section Erreur 3](#erreur-3-valueerror-unsupported-pickle-protocol) |
| Crash epoch 99 | [Section Erreur 4](#erreur-4-crash-√†-epoch-99) |

### Ressources

- **Article original** : https://arxiv.org/abs/2112.12047
- **GitHub** : https://github.com/jli0117/ehrMGAN
- **eICU Dataset** : https://physionet.org/content/eicu-crd-demo/2.0.1/
- **Issues** : https://github.com/jli0117/ehrMGAN/issues

---

**Auteur** : [Votre Nom]
**Version** : 2.0
**Derni√®re mise √† jour** : F√©vrier 2026
**Licence** : MIT

---

*Bon entra√Ænement ! üöÄ*
