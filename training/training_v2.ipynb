{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial for training EHR-M-GAN (Version Corrig√©e)\n",
    "\n",
    "Reference: \"Generating Synthetic Mixed-type Longitudinal Electronic Health Records for Artificial Intelligent\". https://arxiv.org/abs/2112.12047\n",
    "\n",
    "In this work, we propose a generative adversarial network (GAN) entitled EHR-M-GAN which simultaneously synthesizes mixed-type timeseries EHR data (e.g., continuous-valued timeseries and discrete-valued timeseries). EHR-M-GAN is capable of capturing the multidimensional, heterogeneous, and correlated temporal dynamics in patient trajectories.\n",
    "\n",
    "---\n",
    "**CORRECTIONS APPORT√âES :**\n",
    "- ‚úÖ Ajout du chemin du projet au PYTHONPATH\n",
    "- ‚úÖ Chemins de donn√©es corrig√©s pour Windows\n",
    "- ‚úÖ Compatibilit√© TensorFlow 1.x/2.x\n",
    "- ‚úÖ V√©rifications des fichiers avant chargement\n",
    "- ‚úÖ Nom de fichier corrig√© (statics.pkl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\work\\\\ehrMGAN-main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-431532651.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Naviguer vers le projet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPROJECT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"D:\\work\\ehrMGAN-main\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Ajouter au PYTHONPATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\work\\\\ehrMGAN-main'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Naviguer vers le projet\n",
    "PROJECT_PATH = r\"D:\\work\\ehrMGAN-main\"\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Ajouter au PYTHONPATH\n",
    "if PROJECT_PATH not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "# V√©rifier\n",
    "assert os.path.exists('networks.py'), \"‚ùå networks.py introuvable!\"\n",
    "print(f\"‚úÖ Projet configur√©: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chemin du projet ajout√©: D:\\work\\ehrMGAN-main\n",
      "‚úÖ R√©pertoire de travail actuel: /content\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# IMPORTANT: Ajuster ce chemin selon votre installation\n",
    "# Pour Windows: r\"D:\\work\\ehrMGAN-main\"\n",
    "# Pour Linux/Mac: \"/path/to/ehrMGAN-main\"\n",
    "PROJECT_PATH = r\"D:\\work\\ehrMGAN-main\"\n",
    "\n",
    "# Ajouter le chemin du projet au PYTHONPATH pour importer les modules\n",
    "if PROJECT_PATH not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "print(f\"‚úÖ Chemin du projet ajout√©: {PROJECT_PATH}\")\n",
    "print(f\"‚úÖ R√©pertoire de travail actuel: {os.getcwd()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary packages and functions call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "‚ùå Erreur d'import: No module named 'networks'\n",
      "V√©rifiez que vous √™tes dans le bon r√©pertoire et que tous les fichiers sont pr√©sents.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1626065942.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Import des modules du projet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mC_VAE_NET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_VAE_NET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_GAN_NET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_GAN_NET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mm3gan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mm3gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrenormlizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networks'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# V√©rifier la version de TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Import des modules du projet\n",
    "try:\n",
    "    from networks import C_VAE_NET, D_VAE_NET, C_GAN_NET, D_GAN_NET\n",
    "    from m3gan import m3gan\n",
    "    from utils import renormlizer\n",
    "    print(\"‚úÖ Tous les modules ont √©t√© import√©s avec succ√®s!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"V√©rifiez que vous √™tes dans le bon r√©pertoire et que tous les fichiers sont pr√©sents.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins de donn√©es\n",
    "# IMPORTANT: Ajuster ces chemins selon votre configuration\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, \"data\", \"real\", \"eicu\")\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_PATH, \"data\", \"checkpoint\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_PATH, \"data\", \"fake\")\n",
    "\n",
    "print(f\"üìÅ Chemin des donn√©es: {DATA_PATH}\")\n",
    "print(f\"üíæ Chemin des checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"üìä Chemin de sortie: {OUTPUT_DIR}\")\n",
    "\n",
    "# Cr√©er les r√©pertoires s'ils n'existent pas\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"‚úÖ R√©pertoires v√©rifi√©s/cr√©√©s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset\n",
    "  - MIMIC-III: https://physionet.org/content/mimiciii/1.4/\n",
    "  - eICU-CRD: https://physionet.org/content/eicu-crd/2.0/ (used in this tutorial)\n",
    "  - HiRID: https://physionet.org/content/hirid/1.1.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier l'existence des fichiers avant de les charger\n",
    "required_files = [\n",
    "    'vital_sign_24hrs.pkl',\n",
    "    'med_interv_24hrs.pkl',\n",
    "    'statics.pkl'  # Corrig√©: √©tait 'statics_cond.pkl'\n",
    "]\n",
    "\n",
    "print(\"V√©rification des fichiers de donn√©es...\")\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(DATA_PATH, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  ‚úÖ {filename} trouv√©\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {filename} MANQUANT √† {filepath}\")\n",
    "        print(f\"\\n‚ö†Ô∏è  ERREUR: Fichiers de donn√©es manquants!\")\n",
    "        print(f\"Veuillez vous assurer que les fichiers suivants existent dans {DATA_PATH}:\")\n",
    "        for f in required_files:\n",
    "            print(f\"  - {f}\")\n",
    "        raise FileNotFoundError(f\"Fichier manquant: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es pour l'entra√Ænement du GAN\n",
    "print(\"Chargement des donn√©es...\")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'vital_sign_24hrs.pkl'), 'rb') as f:\n",
    "    vital_labs_3D = pickle.load(f)\n",
    "    print(f\"  ‚úÖ vital_sign_24hrs.pkl charg√© - Shape: {vital_labs_3D.shape}\")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'med_interv_24hrs.pkl'), 'rb') as f:\n",
    "    medical_interv_3D = pickle.load(f)\n",
    "    print(f\"  ‚úÖ med_interv_24hrs.pkl charg√© - Shape: {medical_interv_3D.shape}\")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'statics.pkl'), 'rb') as f:\n",
    "    statics = pickle.load(f)\n",
    "    print(f\"  ‚úÖ statics.pkl charg√© - Type: {type(statics)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Toutes les donn√©es ont √©t√© charg√©es avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_x = vital_labs_3D\n",
    "discrete_x = medical_interv_3D\n",
    "\n",
    "print(f\"Donn√©es continues (vital signs): {continuous_x.shape}\")\n",
    "print(f\"Donn√©es discr√®tes (interventions m√©dicales): {discrete_x.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres des s√©ries temporelles\n",
    "time_steps = continuous_x.shape[1]\n",
    "c_dim = continuous_x.shape[2]\n",
    "d_dim = discrete_x.shape[2]\n",
    "no_gen = continuous_x.shape[0]\n",
    "\n",
    "print(f\"üìä Param√®tres des donn√©es:\")\n",
    "print(f\"  - Time steps: {time_steps}\")\n",
    "print(f\"  - Continuous dimensions: {c_dim}\")\n",
    "print(f\"  - Discrete dimensions: {d_dim}\")\n",
    "print(f\"  - Number of samples: {no_gen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres de la phase VAE (pr√©-entra√Ænement)\n",
    "c_noise_dim = 5\n",
    "c_z_size = 100\n",
    "d_noise_dim = 5\n",
    "d_z_size = 100\n",
    "\n",
    "print(f\"üîß Param√®tres VAE:\")\n",
    "print(f\"  - C noise dim: {c_noise_dim}\")\n",
    "print(f\"  - C latent size: {c_z_size}\")\n",
    "print(f\"  - D noise dim: {d_noise_dim}\")\n",
    "print(f\"  - D latent size: {d_z_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire les r√©seaux VAE et GAN\n",
    "print(\"Construction des r√©seaux neuronaux...\")\n",
    "\n",
    "c_vae = C_VAE_NET(x_dim=c_dim, z_dim=c_z_size, time_steps=time_steps)\n",
    "print(\"  ‚úÖ C_VAE_NET construit\")\n",
    "\n",
    "c_gan = C_GAN_NET(x_dim=c_dim, z_dim=c_z_size, time_steps=time_steps)\n",
    "print(\"  ‚úÖ C_GAN_NET construit\")\n",
    "\n",
    "d_vae = D_VAE_NET(x_dim=d_dim, z_dim=d_z_size, time_steps=time_steps)\n",
    "print(\"  ‚úÖ D_VAE_NET construit\")\n",
    "\n",
    "d_gan = D_GAN_NET(x_dim=d_dim, z_dim=d_z_size, time_steps=time_steps)\n",
    "print(\"  ‚úÖ D_GAN_NET construit\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les r√©seaux ont √©t√© construits avec succ√®s!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres d'entra√Ænement\n",
    "batch_size = 128\n",
    "num_pre_epochs = 100  # Pr√©-entra√Ænement VAE\n",
    "num_epochs = 500      # Entra√Ænement GAN complet\n",
    "\n",
    "# Fr√©quences de sauvegarde\n",
    "epoch_ckpt_freq = 100  # Sauvegarder un checkpoint tous les 100 epochs\n",
    "epoch_loss_freq = 10   # Afficher les pertes tous les 10 epochs\n",
    "\n",
    "print(f\"‚öôÔ∏è Param√®tres d'entra√Ænement:\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Pre-training epochs: {num_pre_epochs}\")\n",
    "print(f\"  - Training epochs: {num_epochs}\")\n",
    "print(f\"  - Checkpoint frequency: {epoch_ckpt_freq}\")\n",
    "print(f\"  - Loss display frequency: {epoch_loss_freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounds pour discriminateur, g√©n√©rateur et VAE\n",
    "d_rounds = 1\n",
    "g_rounds = 3\n",
    "v_rounds = 1\n",
    "\n",
    "print(f\"üîÑ Rounds d'entra√Ænement:\")\n",
    "print(f\"  - Discriminator rounds: {d_rounds}\")\n",
    "print(f\"  - Generator rounds: {g_rounds}\")\n",
    "print(f\"  - VAE rounds: {v_rounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rates\n",
    "v_lr_pre = 1e-3  # Learning rate pour pr√©-entra√Ænement VAE\n",
    "v_lr = 1e-4      # Learning rate pour VAE pendant GAN\n",
    "g_lr = 1e-4      # Learning rate pour g√©n√©rateur\n",
    "d_lr = 1e-4      # Learning rate pour discriminateur\n",
    "\n",
    "print(f\"üìà Learning rates:\")\n",
    "print(f\"  - VAE pre-training: {v_lr_pre}\")\n",
    "print(f\"  - VAE: {v_lr}\")\n",
    "print(f\"  - Generator: {g_lr}\")\n",
    "print(f\"  - Discriminator: {d_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients de perte\n",
    "alpha_re = 1.0   # Reconstruction loss\n",
    "alpha_kl = 1.0   # KL divergence\n",
    "alpha_mt = 1.0   # Marginal temporal loss\n",
    "alpha_ct = 1.0   # Conditional temporal loss\n",
    "alpha_sm = 1.0   # Semantic loss\n",
    "\n",
    "c_beta_adv = 1.0  # Adversarial loss pour continuous\n",
    "c_beta_fm = 10.0  # Feature matching loss pour continuous\n",
    "d_beta_adv = 1.0  # Adversarial loss pour discrete\n",
    "d_beta_fm = 10.0  # Feature matching loss pour discrete\n",
    "\n",
    "print(f\"‚öñÔ∏è Coefficients de perte:\")\n",
    "print(f\"  VAE:\")\n",
    "print(f\"    - Reconstruction (alpha_re): {alpha_re}\")\n",
    "print(f\"    - KL divergence (alpha_kl): {alpha_kl}\")\n",
    "print(f\"    - Marginal temporal (alpha_mt): {alpha_mt}\")\n",
    "print(f\"    - Conditional temporal (alpha_ct): {alpha_ct}\")\n",
    "print(f\"    - Semantic (alpha_sm): {alpha_sm}\")\n",
    "print(f\"  GAN Continuous:\")\n",
    "print(f\"    - Adversarial (c_beta_adv): {c_beta_adv}\")\n",
    "print(f\"    - Feature matching (c_beta_fm): {c_beta_fm}\")\n",
    "print(f\"  GAN Discrete:\")\n",
    "print(f\"    - Adversarial (d_beta_adv): {d_beta_adv}\")\n",
    "print(f\"    - Feature matching (d_beta_fm): {d_beta_fm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du checkpoint\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "print(f\"üíæ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "**IMPORTANT:** Ce code g√®re automatiquement la compatibilit√© entre TensorFlow 1.x et 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier la version de TensorFlow et configurer en cons√©quence\n",
    "tf_version = int(tf.__version__.split('.')[0])\n",
    "print(f\"\\nüîç TensorFlow version d√©tect√©e: {tf.__version__} (v{tf_version})\")\n",
    "\n",
    "if tf_version == 1:\n",
    "    print(\"‚úÖ Utilisation de TensorFlow 1.x\")\n",
    "    # Code pour TensorFlow 1.x (original)\n",
    "    tf.reset_default_graph()\n",
    "    run_config = tf.ConfigProto()\n",
    "    run_config.gpu_options.allow_growth = True\n",
    "    \n",
    "    with tf.Session(config=run_config) as sess:\n",
    "        model = m3gan(sess=sess,\n",
    "                      batch_size=batch_size,\n",
    "                      time_steps=time_steps,\n",
    "                      num_pre_epochs=num_pre_epochs,\n",
    "                      num_epochs=num_epochs,\n",
    "                      checkpoint_dir=checkpoint_dir,\n",
    "                      epoch_ckpt_freq=epoch_ckpt_freq,\n",
    "                      epoch_loss_freq=epoch_loss_freq,\n",
    "                      # params for c\n",
    "                      c_dim=c_dim, c_noise_dim=c_noise_dim,\n",
    "                      c_z_size=c_z_size, c_data_sample=continuous_x,\n",
    "                      c_vae=c_vae, c_gan=c_gan,\n",
    "                      # params for d\n",
    "                      d_dim=d_dim, d_noise_dim=d_noise_dim,\n",
    "                      d_z_size=d_z_size, d_data_sample=discrete_x,\n",
    "                      d_vae=d_vae, d_gan=d_gan,\n",
    "                      # params for training\n",
    "                      d_rounds=d_rounds, g_rounds=g_rounds, v_rounds=v_rounds,\n",
    "                      v_lr_pre=v_lr_pre, v_lr=v_lr, g_lr=g_lr, d_lr=d_lr,\n",
    "                      alpha_re=alpha_re, alpha_kl=alpha_kl, alpha_mt=alpha_mt, \n",
    "                      alpha_ct=alpha_ct, alpha_sm=alpha_sm,\n",
    "                      c_beta_adv=c_beta_adv, c_beta_fm=c_beta_fm, \n",
    "                      d_beta_adv=d_beta_adv, d_beta_fm=d_beta_fm)\n",
    "        model.build()\n",
    "        model.train()\n",
    "        \n",
    "elif tf_version == 2:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow 2.x d√©tect√©\")\n",
    "    print(\"Ce code a √©t√© √©crit pour TensorFlow 1.x\")\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"  1. Installer TensorFlow 1.15: pip install tensorflow==1.15\")\n",
    "    print(\"  2. Utiliser le mode de compatibilit√© TF2:\")\n",
    "    print(\"\")\n",
    "    print(\"import tensorflow.compat.v1 as tf\")\n",
    "    print(\"tf.disable_v2_behavior()\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Tentative d'utiliser le mode de compatibilit√©\n",
    "    try:\n",
    "        import tensorflow.compat.v1 as tf\n",
    "        tf.disable_v2_behavior()\n",
    "        print(\"\\n‚úÖ Mode de compatibilit√© TF1 activ√©\")\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        run_config = tf.ConfigProto()\n",
    "        run_config.gpu_options.allow_growth = True\n",
    "        \n",
    "        with tf.Session(config=run_config) as sess:\n",
    "            model = m3gan(sess=sess,\n",
    "                          batch_size=batch_size,\n",
    "                          time_steps=time_steps,\n",
    "                          num_pre_epochs=num_pre_epochs,\n",
    "                          num_epochs=num_epochs,\n",
    "                          checkpoint_dir=checkpoint_dir,\n",
    "                          epoch_ckpt_freq=epoch_ckpt_freq,\n",
    "                          epoch_loss_freq=epoch_loss_freq,\n",
    "                          # params for c\n",
    "                          c_dim=c_dim, c_noise_dim=c_noise_dim,\n",
    "                          c_z_size=c_z_size, c_data_sample=continuous_x,\n",
    "                          c_vae=c_vae, c_gan=c_gan,\n",
    "                          # params for d\n",
    "                          d_dim=d_dim, d_noise_dim=d_noise_dim,\n",
    "                          d_z_size=d_z_size, d_data_sample=discrete_x,\n",
    "                          d_vae=d_vae, d_gan=d_gan,\n",
    "                          # params for training\n",
    "                          d_rounds=d_rounds, g_rounds=g_rounds, v_rounds=v_rounds,\n",
    "                          v_lr_pre=v_lr_pre, v_lr=v_lr, g_lr=g_lr, d_lr=d_lr,\n",
    "                          alpha_re=alpha_re, alpha_kl=alpha_kl, alpha_mt=alpha_mt, \n",
    "                          alpha_ct=alpha_ct, alpha_sm=alpha_sm,\n",
    "                          c_beta_adv=c_beta_adv, c_beta_fm=c_beta_fm, \n",
    "                          d_beta_adv=d_beta_adv, d_beta_fm=d_beta_fm)\n",
    "            model.build()\n",
    "            model.train()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur avec le mode de compatibilit√©: {e}\")\n",
    "        print(\"\\nVeuillez installer TensorFlow 1.15:\")\n",
    "        print(\"  pip uninstall tensorflow\")\n",
    "        print(\"  pip install tensorflow==1.15\")\n",
    "else:\n",
    "    raise ValueError(f\"Version TensorFlow non support√©e: {tf.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V√©rification des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier que les donn√©es synth√©tiques ont √©t√© g√©n√©r√©es\n",
    "import glob\n",
    "\n",
    "synthetic_files = glob.glob(os.path.join(OUTPUT_DIR, \"epoch*/gen_data.npz\"))\n",
    "print(f\"\\nüìä Fichiers de donn√©es synth√©tiques g√©n√©r√©s: {len(synthetic_files)}\")\n",
    "\n",
    "if synthetic_files:\n",
    "    print(\"\\n‚úÖ Entra√Ænement termin√© avec succ√®s!\")\n",
    "    print(\"\\nFichiers g√©n√©r√©s:\")\n",
    "    for f in sorted(synthetic_files):\n",
    "        print(f\"  - {os.path.basename(os.path.dirname(f))}/gen_data.npz\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Aucun fichier de donn√©es synth√©tiques trouv√©\")\n",
    "    print(\"V√©rifiez que l'entra√Ænement s'est termin√© correctement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
